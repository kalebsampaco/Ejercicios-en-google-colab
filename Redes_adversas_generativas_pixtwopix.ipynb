{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Redes adversas generativas pixtwopix.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalebsampaco/Ejercicios-en-google-colab/blob/master/Redes_adversas_generativas_pixtwopix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jQnm6K4IVCT",
        "colab_type": "code",
        "outputId": "cd83b853-ed0b-425f-9a94-99377a8aa12a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "from __future__ import print_function, division\n",
        "import scipy\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/pix2pix/')\n",
        "import os\n",
        "import cv2 as cv\n",
        "\n",
        "#CLase preparada para carga de datos\n",
        "class Cargador_Datos():\n",
        "    def __init__(self, nombre_dataset, resolucion_imagen=(128, 128)):\n",
        "        self.nombre_dataset = nombre_dataset\n",
        "        self.resolucion_imagen = resolucion_imagen\n",
        "    #Funcion de carga de datos\n",
        "    def carga_datos(self, tamano_batch=1, is_testing=False):\n",
        "        tipo_datos = \"train\" if not is_testing else \"test\"\n",
        "        ruta = glob('/content/drive/My Drive/pix2pix/datasets/%s/%s/*' % (self.nombre_dataset, tipo_datos))\n",
        "\n",
        "        batch_de_imagenes = np.random.choice(ruta, size=tamano_batch)\n",
        "\n",
        "        imagenes_A = []\n",
        "        imagenes_B = []\n",
        "        for ruta_imagen in batch_de_imagenes:\n",
        "            imagen = self.imread(ruta_imagen)\n",
        "\n",
        "            alto, ancho, _ = imagen.shape\n",
        "            _ancho = int(ancho/2)\n",
        "            imagen_A, imagen_B = imagen[:, :_ancho, :], imagen[:, _ancho:, :]\n",
        "\n",
        "            imagen_A = cv.resize(imagen_A, self.resolucion_imagen)\n",
        "            imagen_B = cv.resize(imagen_B, self.resolucion_imagen)\n",
        "\n",
        "            # Aumento de datos solo en entrenamiento\n",
        "            if not is_testing and np.random.random() < 0.5:\n",
        "                imagen_A = np.fliplr(imagen_A)\n",
        "                imagen_B = np.fliplr(imagen_B)\n",
        "\n",
        "            imagenes_A.append(imagen_A)\n",
        "            imagenes_B.append(imagen_B)\n",
        "\n",
        "        imagenes_A = np.array(imagenes_A)/127.5 - 1.\n",
        "        imagenes_B = np.array(imagenes_B)/127.5 - 1.\n",
        "\n",
        "        return imagenes_A, imagenes_B\n",
        "      \n",
        "    #Funcion de ccarga de batches\n",
        "    def carga_batch(self, tamano_batch=1, is_testing=False):\n",
        "        tipo_datos = \"train\" if not is_testing else \"val\"\n",
        "        ruta = glob('/content/drive/My Drive/pix2pix/datasets/%s/%s/*' % (self.nombre_dataset, tipo_datos))\n",
        "\n",
        "        self.numero_batches = int(len(ruta) / tamano_batch)\n",
        "\n",
        "        for i in range(self.numero_batches-1):\n",
        "            batch = ruta[i*tamano_batch:(i+1)*tamano_batch]\n",
        "            imagenes_A, imagenes_B = [], []\n",
        "            for imagen in batch:\n",
        "                imagen = self.imread(imagen)\n",
        "                alto, ancho, _ = imagen.shape\n",
        "                _ancho = int(ancho/2)\n",
        "                imagen_A = imagen[:, :_ancho, :]\n",
        "                imagen_B = imagen[:, _ancho:, :]\n",
        "              \n",
        "                imagen_A = cv.resize(imagen_A, self.resolucion_imagen)\n",
        "                imagen_B = cv.resize(imagen_B, self.resolucion_imagen)\n",
        "\n",
        "                if not is_testing and np.random.random() > 0.5:\n",
        "                        imagen_A = np.fliplr(imagen_A)\n",
        "                        imagen_B = np.fliplr(imagen_B)\n",
        "\n",
        "                imagenes_A.append(imagen_A)\n",
        "                imagenes_B.append(imagen_B)\n",
        "\n",
        "            imagenes_A = np.array(imagenes_A)/127.5 - 1.\n",
        "            imagenes_B = np.array(imagenes_B)/127.5 - 1.\n",
        "\n",
        "            yield imagenes_A, imagenes_B\n",
        "\n",
        "\n",
        "    def imread(self, ruta):\n",
        "        return cv.imread(ruta)\n",
        "\n",
        "#Definicion de Clase GAN Pix2Pix\n",
        "class Pix2Pix():\n",
        "    def __init__(self):\n",
        "        # Tamaño de entrada de imagenes\n",
        "        self.filas_imagen = 256\n",
        "        self.columnas_imagen = 256\n",
        "        self.canales = 3\n",
        "        self.tamano_imagen = (self.filas_imagen, self.columnas_imagen, self.canales)\n",
        "\n",
        "        # Configuracion de base de datos y cargador de datos\n",
        "        self.nombre_dataset = 'facades'\n",
        "        self.cargador_datos = Cargador_Datos(nombre_dataset=self.nombre_dataset,resolucion_imagen=(self.filas_imagen, self.columnas_imagen))\n",
        "\n",
        "\n",
        "        # Calculo de salidas del discriminador\n",
        "        parche = int(self.filas_imagen / 2**4)\n",
        "        self.parches_discriminador = (parche, parche, 1)\n",
        "\n",
        "        # Numero de filtros iniciales de discriminador y generador\n",
        "        self.gf = 64\n",
        "        self.df = 64\n",
        "\n",
        "        optimizador = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Construccion y compilación del discriminador\n",
        "        self.discriminador = self.crea_discriminador()\n",
        "        self.discriminador.compile(loss='mse',optimizer=optimizador,metrics=['accuracy'])\n",
        "\n",
        "        # Construccion de generador\n",
        "        self.generador = self.crea_generador()\n",
        "\n",
        "        # Entradas y condiciones de entrada\n",
        "        imagen_A = Input(shape=self.tamano_imagen)\n",
        "        imagen_B = Input(shape=self.tamano_imagen)\n",
        "\n",
        "        # Condicionado mediante B para generar una falsa A\n",
        "        falsa_A = self.generador(imagen_B)\n",
        "\n",
        "        # Congelado del discriminador en el modelo conjunto\n",
        "        self.discriminador.trainable = False\n",
        "\n",
        "        # Validez de imagenes calculadas a través del discriminador\n",
        "        valido = self.discriminador([falsa_A, imagen_B])\n",
        "\n",
        "        self.combinado = Model(inputs=[imagen_A, imagen_B], outputs=[valido, falsa_A])\n",
        "        self.combinado.compile(loss=['mse', 'mae'],loss_weights=[1, 100],optimizer=optimizador)\n",
        "\n",
        "    #Funcion de creacion del generador\n",
        "    def crea_generador(self):   \n",
        "        \n",
        "        #definicion de convolucion+Leaky Relu\n",
        "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
        "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "            d = LeakyReLU(alpha=0.2)(d)\n",
        "            if bn:\n",
        "                d = BatchNormalization(momentum=0.8)(d)\n",
        "            return d\n",
        "        \n",
        "        #definicion de aproximacion a deconvolucion\n",
        "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "            u = UpSampling2D(size=2)(layer_input)\n",
        "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
        "            if dropout_rate:\n",
        "                u = Dropout(dropout_rate)(u)\n",
        "            u = BatchNormalization(momentum=0.8)(u)\n",
        "            u = Concatenate()([u, skip_input])\n",
        "            return u\n",
        "\n",
        "        # Entrada\n",
        "        d0 = Input(shape=self.tamano_imagen)\n",
        "\n",
        "        # Encoder\n",
        "        d1 = conv2d(d0, self.gf, bn=False)\n",
        "        d2 = conv2d(d1, self.gf*2)\n",
        "        d3 = conv2d(d2, self.gf*4)\n",
        "        d4 = conv2d(d3, self.gf*8)\n",
        "        d5 = conv2d(d4, self.gf*8)\n",
        "        d6 = conv2d(d5, self.gf*8)\n",
        "        d7 = conv2d(d6, self.gf*8)\n",
        "\n",
        "        # Decoder\n",
        "        u1 = deconv2d(d7, d6, self.gf*8)\n",
        "        u2 = deconv2d(u1, d5, self.gf*8)\n",
        "        u3 = deconv2d(u2, d4, self.gf*8)\n",
        "        u4 = deconv2d(u3, d3, self.gf*4)\n",
        "        u5 = deconv2d(u4, d2, self.gf*2)\n",
        "        u6 = deconv2d(u5, d1, self.gf)\n",
        "        u7 = UpSampling2D(size=2)(u6)\n",
        "        imagen_salida = Conv2D(self.canales, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
        "\n",
        "        return Model(d0, imagen_salida)\n",
        "\n",
        "     #Funcion de creacion del discriminador\n",
        "    def crea_discriminador(self):\n",
        "\n",
        "        #definicion de capa de discriminacion\n",
        "        def d_layer(layer_input, filters, f_size=4, bn=True):\n",
        "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "            d = LeakyReLU(alpha=0.2)(d)\n",
        "            if bn:\n",
        "                d = BatchNormalization(momentum=0.8)(d)\n",
        "            return d\n",
        "         \n",
        "        #definicion de entradas\n",
        "        imagen_A = Input(shape=self.tamano_imagen)\n",
        "        imagen_B = Input(shape=self.tamano_imagen)\n",
        "\n",
        "        # Concatenacion de imagen y condicion en canales\n",
        "        combined_imgs = Concatenate(axis=-1)([imagen_A, imagen_B])\n",
        "\n",
        "        d1 = d_layer(combined_imgs, self.df, bn=False)\n",
        "        d2 = d_layer(d1, self.df*2)\n",
        "        d3 = d_layer(d2, self.df*4)\n",
        "        d4 = d_layer(d3, self.df*8)\n",
        "        \n",
        "        #obtencion de validez a traves del discriminador\n",
        "        validez = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
        "\n",
        "        return Model([imagen_A, imagen_B], validez)\n",
        "\n",
        "    # Funcion de entrenamiento  \n",
        "    def train(self, epocas, tamano_batch=1, intervalo_muestra=50):\n",
        "\n",
        "        inicio = datetime.datetime.now()\n",
        "\n",
        "        # Definicion de etiquetas validas y falsas para entrenar\n",
        "        valido = np.ones((tamano_batch,) + self.parches_discriminador)\n",
        "        falsa = np.zeros((tamano_batch,) + self.parches_discriminador)\n",
        "        for epoca in range(epocas):\n",
        "            for batch_i, (imagenes_A, imagenes_B) in enumerate(self.cargador_datos.carga_batch(tamano_batch)):\n",
        "\n",
        "\n",
        "                # Generacion de imagen a traves de condicion B\n",
        "                falsa_A = self.generador.predict(imagenes_B)\n",
        "\n",
        "                # Entrenamiento del discriminador\n",
        "                perdidas_discriminador_real = self.discriminador.train_on_batch([imagenes_A, imagenes_B], valido)\n",
        "                perdidas_discriminador_falsa = self.discriminador.train_on_batch([falsa_A, imagenes_B], falsa)\n",
        "                perdidas_discriminador = 0.5 * np.add(perdidas_discriminador_real, perdidas_discriminador_falsa)\n",
        "\n",
        "                #Entrenamiento del generador\n",
        "                perdidas_generador = self.combinado.train_on_batch([imagenes_A, imagenes_B], [valido, imagenes_A])\n",
        "\n",
        "                tiempo_final = datetime.datetime.now() - inicio\n",
        "\n",
        "\n",
        "                # Guardado y ploteo cada intervalo de muestra\n",
        "                if batch_i % intervalo_muestra == 0:\n",
        "                    print (\"[epoca %d/%d] [Batch %d/%d] [perdidas discriminador: %f, precision: %3d%%] [perdidas generador: %f] tiempo: %s\" % (epoca, epocas,\n",
        "                                                                batch_i, self.cargador_datos.numero_batches,perdidas_discriminador[0], 100*perdidas_discriminador[1],\n",
        "                                                                perdidas_generador[0],tiempo_final))\n",
        "                    self.combinado.save_weights('/content/drive/My Drive/srgan/saved_model/combined.h5')\n",
        "                    self.discriminador.save_weights('/content/drive/My Drive/srgan/saved_model/discriminator.h5')\n",
        "                    self.generador.save_weights('/content/drive/My Drive/srgan/saved_model/generator.h5')\n",
        "                    self.imagenes_muestra(epoca, batch_i)\n",
        "\n",
        "    #Funcion de ploteo de imagenes de muestra\n",
        "    def imagenes_muestra(self, epoca, batch_i):\n",
        "        r, c = 3, 3\n",
        "\n",
        "        imagenes_A, imagenes_B = self.cargador_datos.carga_datos(tamano_batch=3, is_testing=True)\n",
        "        falsa_A = self.generador.predict(imagenes_B)\n",
        "\n",
        "        imagenes_generadas = np.concatenate([imagenes_B, falsa_A, imagenes_A])\n",
        "\n",
        "        imagenes_generadas = 0.5 * imagenes_generadas + 0.5\n",
        "\n",
        "        titulos = ['Condicion', 'Generada', 'Original']\n",
        "        fiigura, ejes = plt.subplots(r, c)\n",
        "        contador = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                ejes[i,j].imshow(imagenes_generadas[contador])\n",
        "                ejes[i, j].set_title(titulos[i])\n",
        "                ejes[i,j].axis('off')\n",
        "                contador += 1\n",
        "        plt.plot()\n",
        "\n",
        "\n",
        "gan = Pix2Pix()\n",
        "\n",
        "gan.train(epocas=200, tamano_batch=5, intervalo_muestra=100)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}